{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b07136-3799-4877-bce5-dcf7a030df89",
   "metadata": {},
   "source": [
    "### Evaluation of Named Entity Linking for Dutch\n",
    "\n",
    "\n",
    "| System  | Damuel | Mewsli-X | WiNNL | MultiNERD|\n",
    "| ----- | ------ | ------ | ------ | ------- | \n",
    "| Spacy+Spotlight  |   | | 0.397/0.453/0.423\n",
    "| Spacy+wikidata entity finder   |0.391/0.491/0.435 | | 0.589/0.495/0.538 |  0.517/0.602/0.556\n",
    "| BabelFy | | \n",
    "| BELA | |\n",
    "| mGenre | |\n",
    "| mRefined | | \n",
    "\n",
    " - Scores are for Precision/Recall/F-score\n",
    " - MultiNERD: over first 1000 lines (52 sentences), with filtering on NE types\n",
    " - WiNNL: over 100 items, 4124-4224, some 404 issues in annotation (those sentences are ignored), also does not annotate dates (it seems), also: evaluating single sentences underperforms as previous sent often contains full name \n",
    " - damuel : over first 100 articles (not all with text), no NE types, so filtering for named entities only is very hard. Also: on full texts with lots of repeated names, evaluation on unique qids (ie using sets) might give misleading scores.\n",
    " - damuel with NEC filtering: precision:0.308, recall:0.463, fscore:0.370, with NORP class (noisy): precision:0.273, recall:0.474, fscore:0.346 This is even lower than the old approximate approach evaluating only on strings that contain a PROPN pos-tag. Could it be improved?\n",
    " - damuel with lang=nl, with NORP, with filtering for family name:  precision:0.299, recall:0.516, fscore:0.378 it seems switching to nl language has big effect (was mistake in code to set it to en), words like 'Niettegenstaande' now also labeled, many NORP cases, many no_qid_found (exclude from evaluation?) without NORP: precision:0.336, recall:0.505, fscore:0.403, ignore no_qid_found: precision:0.358, recall:0.505, fscore:0.419\n",
    "\n",
    "GERBIL as evaluation platform? (but seems complicated), inKB recall, accuracy (for experiments where entities are given)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b629de8-9300-4713-9aac-a4b4133b0593",
   "metadata": {},
   "source": [
    "## Evaluation Datasets \n",
    "\n",
    "### WiNNL\n",
    "\n",
    "1500 sentences of Wikinews, \n",
    "\n",
    "_WiNNLâ€™s annotation scheme prioritises three core categories of entities: PER, ORG and LOC._ While PER,LOC,ORG are the most frequent, there are also 79 entities from other classes (see stats below). For now, it seems best to include all annotated entities in the evaluation, while for an automatic system, we might want to keep only PER, LOC, and ORG entities for evaluation (thus potentially increasing precision). \n",
    "\n",
    "| count | tag |\n",
    "| ------------- | ------------- |\n",
    "| 2150 | total (B-Q) |\n",
    "|   745 | B-PER |\n",
    "|    678 | B-ORG |\n",
    "| 648 | B-LOC |\n",
    "|     37| B-OTH|\n",
    "|    13 | B-EVT |\n",
    "|     10 | B-DATE |\n",
    "|      8 | B-AMB |\n",
    " |     7 | B-SPE |\n",
    "  |    4 | B-DISEASE |\n",
    "  \n",
    "\n",
    "_The system scans through all n-grams of the article text and creates offset-based annotations for each combination of n-grams that\n",
    "matches one of the recognised aliases._ Note that this should probably be taken into account as well. I.e. if the data contains both _Bart De Pauw_ and _De Pauw_, ensure that _De Pauw_ is linked to same QID as full name. (If not, it might be linked to another person or to the family name). \n",
    "\n",
    "The  article url can be used to ensure that we evaluate on full article texts in this way. (Note that escape \\ needs to be removed from string, encoding of diacritics and other special chars is apparently no problem). \n",
    "\n",
    "### issues\n",
    "\n",
    "Note that the most frequent QID is Q404 which is the wikidata page describing the http error for non existing pages. This is clearly a mistake in the annotation process. Either we manually correct these cases (see mail from developers) or ignore these in evaluation (easiest might be to just skip all sentences with a 404 error?)\n",
    "\n",
    "Some sentences have no B- items annotated (in spite of the fact that paper says that only sentences with a NE are preserved.) Ignore these as well? Or just score them (effect on macro recall is zero, on macro precision could be negative if the system over-annotates.)\n",
    "\n",
    "\n",
    "### dev set\n",
    "\n",
    "Either use only up to first 500 items or so (4123 - 4622) to ensure that we do not evaluate on test set, or else (better) do it on the basis of article urls, so we can do some global optimization for partial names (see above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77532b94-72ec-4773-ae53-dec5716a9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "winnl = pandas.read_json('WiNNL/dutch_winnl_data.json')\n",
    "\n",
    "winnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b763c7-b70e-4e58-963e-24012ec05ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "winnl.loc[winnl['url'] == \"https://nl.wikinews.org/wiki/Allerlaatste_voor_1900_geboren_persoon_overleden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7f9f7-ac3b-4a94-922c-ab67346d2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://nl.wikinews.org/wiki/Allerlaatste_voor_1900_geboren_persoon_overleden\"\n",
    "#url = \"https://nl.wikinews.org/wiki/Catalaans_president_Puigdemont_gevlucht_naar_Belgi%C3%AB\"\n",
    "url = \"https://nl.wikinews.org/wiki/Al_Jazeera:_%27poging_tot_vliegtuigkaping_verijdeld%27\"\n",
    "\n",
    "for index,row in winnl.loc[winnl['url'] == url].iterrows() :\n",
    "    gold = set()\n",
    "    for token in row['qid'] :\n",
    "        if token.startswith('B-') :\n",
    "            id = token.replace('B-','')\n",
    "            gold.add(id)        \n",
    "    print(row['original'],gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed491543-f495-48b4-935c-01881c6bcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# collect urls and count sentences from same article\n",
    "urls = defaultdict(int)\n",
    "\n",
    "for index,row in winnl.iterrows() :\n",
    "    urls[row['url']] += 1 \n",
    "\n",
    "for (key,val) in urls.items() :\n",
    "    print(key,val)\n",
    "\n",
    "# now collect subset with total of 500 sentences (for development and initial evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96cb1f-a969-4abf-a808-8b46ad4d901c",
   "metadata": {},
   "source": [
    "### MultiNERD\n",
    "\n",
    "https://github.com/Babelscape/multinerd\n",
    "\n",
    "MultiNERD: A Multilingual, Multi-Genre and Fine-Grained Dataset for Named Entity Recognition (and Disambiguation).\n",
    "\n",
    "silver quality entity linking corpus, including dutch\n",
    "\n",
    "Alignment of multiNERD and Spacy NE classes (NERD counts from nl_dev.tsv)\n",
    "\n",
    "| Count         | NERD  | Spacy | Notes \n",
    "| ------- | ------------- | --- | ---- |\n",
    " |  9336 | B-LOC | LOC, GPE | \n",
    " | 4865 | B-PER | PERSON |\n",
    " |  3587 | B-TIME | TIME, DATE| could include CARDINAL as well\n",
    " | 3256 | B-ANIM | | animals, mostly lower case \n",
    " |  1281 | B-ORG | ORG |\n",
    "  |  834 | B-FOOD | | mostly lower case \n",
    "  |  785 | B-PLANT | | mostly lower case \n",
    "   | 671 | B-DIS | | mostly lower case \n",
    "  |  475 | B-EVE | EVENT | \n",
    "  |  337 | B-MEDIA | WORK_OF_ART | Spel Zonder Grenzen, etc \n",
    "  |  163 | B-CEL || celestial bodies, mix of lower (zon) and upper (Mars)\n",
    "| 142 | B-MYTH | PERSON| mythical figure\n",
    "  |   47 |B-VEHI | PRODUCT | vehicle\n",
    "   |  28 | B-BIO ||mostly upper case (latin medical terms)\n",
    "    | 19  | B-INST | mostly upper case| instruments \n",
    "\n",
    "Spacy https://spacy.io/models/nl NE classes: CARDINAL, DATE, EVENT, FAC (facilities), GPE, LANGUAGE, LAW, LOC, MONEY, NORP (national, religious groups), ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART\n",
    "\n",
    "Note that years are frequently confused as DATE or CARDINAL. So we could implement a heuristic where 4-digit CARDINALs are included and resolved to the QID that is of type year, not number. \n",
    "\n",
    "#### What to include in the evaluation? \n",
    "\n",
    "- From NERD ignore those that are not predominantly named entities: ANIM, FOOD, PLANT, DIS\n",
    "- From Space, ignore those that are not annotated by NERD: CARDINAL (?), FAC, LANGUAGE, LAW, MONEY, NORP, ORDINAL, PERCENT, QUANTITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b14cc6-034c-438a-a675-7b2771e1aa17",
   "metadata": {},
   "source": [
    "### Damuel\n",
    "\n",
    "[Damuel](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5047?show=full) consists of two components: a knowledge base that contains language-agnostic information about entities, including their claims from Wikidata and named entity types (PER, ORG, LOC, EVENT, BRAND, WORK_OF_ART, MANUFACTURED); and Wikipedia texts with entity mentions linked to the knowledge base, along with language-specific text from Wikidata such as labels, aliases, and descriptions, stored separately for each language. \n",
    "\n",
    "Note that the KG with just those entities that have a NE type contains over 27M entities. Loading the file with all nec (1.1G) works on colossus. \n",
    "\n",
    "### issues\n",
    "\n",
    "Links are for concepts (_zonnewende, paganistisch, runen, Noorse_), not just named entities (_Heinrich Himmler, Dachau, SS_). But these can be obtained from the KG. So why are things like zonnewende or dates (see below) included?\n",
    "So either just evaluate for precision, or else evaluate on those entities that are labelled as PROPN in the token layer. \n",
    "\n",
    "PROPN only misses 'Jacobus II van Schotland' as not all parts are PROPN....check first token only or reverse the logic: entity if one of the tokens is PROPN. \n",
    "\n",
    "Some dates are included as well, these could be included if we can identify them in the annotation. Note that in the KG they are listed as well, but without NE class. \n",
    "\n",
    "__Alternative__: evaluate on those entities with ne class loc/per/etc. but this requires finding them in the (20G) KG or some clever preprocessing. Extracted all QID/NE type pairs from the KG (in all.nec). You can read this into a dict, with qid as key and list of NE types as value. Reading the data takes a while (therefore saved it as a pickle file, just 600M), but look-up is efficient (much better than hiding qid in a pandas df for instance). Works on colossus, not tested on local workstation yet. \n",
    "\n",
    "As we are evaluating on longer texts, the decision to count only unique links seems somewhat unmotivated. Should we go for all occurrences? (and while we are at it: include string (positions) and evaluate these as well?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58277f97-4ca5-4463-823d-1a3bf21310a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "\n",
    "damuel = pandas.read_json('damuel_1.0_nl/part-00000', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d3ee7-8b4c-4594-a1a9-0aa8f2974c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "damuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3339cb-7078-483c-8b5d-26898fe88936",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = damuel.loc[34]['wiki']\n",
    "print(wiki['text'])\n",
    "# print(wiki['links'])\n",
    "#print(wiki['tokens'])\n",
    "\n",
    "#tokenpos = 0\n",
    "#for token in wiki['tokens'] :\n",
    "#    for link in wiki['links'] :\n",
    "#        if link['start'] == tokenpos :\n",
    "#            try :\n",
    "#                qid = link['qid']\n",
    "#            except :\n",
    "#                qid = 'missing'\n",
    "#            print(token['upostag'], token['lemma'], qid, link['title'])\n",
    "#    tokenpos += 1\n",
    "\n",
    "for link in wiki['links'] :\n",
    "    start = link['start']\n",
    "    end  = link['end']\n",
    "    upostags = []\n",
    "    string = []\n",
    "    propn = 0 \n",
    "    for token in wiki['tokens'][start:end] : \n",
    "        upostags.append(token['upostag'])\n",
    "        string.append(token['lemma'])\n",
    "        if token['upostag'] == 'PROPN' :\n",
    "            propn = 1\n",
    "    try :\n",
    "        qid = link['qid']\n",
    "    except :\n",
    "        qid = 'missing'\n",
    "    #if propn:\n",
    "    print(qid, string, upostags, link['title'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b0a18-c234-4dfc-bb20-f4770881d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "nec_dict = {}\n",
    "for line in open('damuel_1.0_nl/damuel_1.0_wikidata/all.nec') :\n",
    "    nec = json.loads(line)\n",
    "    nec_dict[nec['qid']] = nec['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501ddae-fa5b-4e69-a72c-f714ef3f748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( nec_dict, open( \"damuel_1.0_nl/damuel_1.0_wikidata/all_nec_dict.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f6dd9-9061-4146-80a9-33e5ab9bb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "nec_dict['Q16701841']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4199e63-3cf9-4045-8ed7-4a0a64d2ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "nec_types = defaultdict(list)\n",
    "\n",
    "wiki = damuel.loc[35]['wiki']\n",
    "\n",
    "for link in wiki['links'] :\n",
    "    try :\n",
    "        qid = link['qid']\n",
    "    except :\n",
    "        qid = 'no_qid'\n",
    "    if qid != 'no_qid' :\n",
    "        if nec_types[qid] :\n",
    "            True\n",
    "        else :\n",
    "            try :\n",
    "                nec_types[qid] = nec_dict[qid]\n",
    "            except :\n",
    "                nec_types[qid] = 'no_nec'\n",
    "\n",
    "for key,val in nec_types.items() :\n",
    "    print(key,val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b11554-3a43-4108-9bc5-3e0098a93186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
