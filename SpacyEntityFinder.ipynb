{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5efd9e2-db5a-4c14-a315-461d13775c45",
   "metadata": {},
   "source": [
    "### Spacy NER + Wikidata entity finder API\n",
    "\n",
    "Adopted from a notebook on https://github.com/gossebouma/spacy-wikidata-ner\n",
    "\n",
    "__TODO__: filter results, ie if something is a LOC or GPE according to Spacy, do not return the link for the family name (as it does now for Engeland, Brantes, etc) But on the other hand, NEC is also wrong in many cases, so rigorous filtering might not work either. \n",
    "\n",
    "see https://www.wikidata.org/w/api.php?action=help&modules=wbsearchentities\n",
    "see https://www.wikidata.org/w/api.php?action=help&modules=wbgetentities to get info on a specific QID, like instance or description\n",
    "\n",
    "for instance, with wbgetentities require PERSON qids to be human, ie P31=Q5 (works for Ruud Lubbers, Haary Mulisch, Frits Zernike, Sigrid Kaag), but Microsoft is a software company, business, technology company, public company, Amnesty International is a nongvernamental, nonprofit organisation, nonprofit company, Volkskrant a daily newspaper, Winsum is a populated place, Gemeente Winsum a municipality in the Netherlands, etc. But on the other hand, there are also attributes like inception (for ORG) or located in (for LOC) that might help.\n",
    "\n",
    "this has a pretty detailed example : https://medium.com/@dreamai/linking-extracted-entities-to-wikidata-why-and-how-168eacb4fb87\n",
    "but uses cosine similarity and some other tricks that seem less useful \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7041f-9165-45ff-828e-8cec93714b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('nl_core_news_lg')\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import requests\n",
    "\n",
    "# is it OK to use this globally for a whole session? \n",
    "# yes because disambiguation does not take context or history into account\n",
    "# no if you fiddle with disambiguation strategies \n",
    "cached_spans = {}\n",
    "\n",
    "def wikidata_entity_link(span) :\n",
    "    try :\n",
    "        link = cached_spans[span.text]\n",
    "    except KeyError :\n",
    "        link = wikidata_entity_search(span.text)\n",
    "    return link\n",
    "\n",
    "Span.set_extension('wikidata_id',getter=wikidata_entity_link)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0edb5-b305-43b0-92ac-a84eb1b9ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidata_entity_search(text) :\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', \n",
    "              'language':'nl',\n",
    "              'format':'json',\n",
    "              'search': text}\n",
    "    json = requests.get(url,params).json()\n",
    "    # this part can be replaced by fancier disambiguation methods, or returning a list of ids from all search results\n",
    "    return select_entity(json['search'])\n",
    "\n",
    "def select_entity(results) :\n",
    "    try :\n",
    "        qid = results[0]['id']\n",
    "        if familyname(qid) :\n",
    "            qid = results[1]['id']\n",
    "        else :\n",
    "            True      \n",
    "    except :\n",
    "        qid = 'no_qid_found'\n",
    "    return qid\n",
    "\n",
    "def wikidata_entity_info(qid) :\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbgetentities', \n",
    "              'sites': 'wikidatawiki',\n",
    "              'format':'json',\n",
    "              'ids': qid,\n",
    "               'props':'claims'}\n",
    "    json = requests.get(url,params).json()\n",
    "    return json\n",
    "            \n",
    "def familyname(qid) :\n",
    "    info = wikidata_entity_info(qid)\n",
    "    fam_name = 0\n",
    "    try :\n",
    "        for instance in info['entities'][qid]['claims']['P31'] : #instance \n",
    "            if instance['mainsnak']['datavalue']['value']['id'] == 'Q101352' : #familyname\n",
    "                fam_name = 1\n",
    "    except :\n",
    "        print('no P31')\n",
    "    return fam_name \n",
    "\n",
    "\n",
    "def extract_entities(parse) :   # note that wikidata_entity_link function is called every time we access wikidata_id\n",
    "    # so better to create a copy or results \n",
    "    entities = []\n",
    "    for ent in parse.ents :\n",
    "        entities.append((ent.text,ent.label_,ent._.wikidata_id))\n",
    "    return(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce13f-9bf8-4a6d-b53f-dd12c4a84d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Satellieten zoals de Astra gebruiken elke frequentie twee keer.')\n",
    "\n",
    "entities = extract_entities(doc)\n",
    "for ent in entities:\n",
    "    print(ent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165454cd-0e51-4370-a9eb-2704934d562c",
   "metadata": {},
   "source": [
    "### Annotate and Score\n",
    "\n",
    "General functions for annotation with Spacy and for reporting scores. Spacy NE annotation can be filtered using a given list of NECLASSES as used by Spacy. score is a dict with system, gold and overlap as keys and numeric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158bfa1-b237-4c4e-a0cf-36d13eaece8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_annotation(entities,NECLASSES) :\n",
    "    print(entities)\n",
    "    system = set() # do we ever have multiple identical entities, if so, set functions should not be used?\n",
    "    for ent in entities :  # one case is not-found errors, should they be unique? notfound+string? \n",
    "                           # just ignore no_qid_found completely : not a link so never contributes to P or R \n",
    "        if ent[1] in NECLASSES and ent[2] != 'no_qid_found' : \n",
    "            system.add(ent[2])\n",
    "    return system \n",
    "\n",
    "def update_score(system,gold,score) :\n",
    "    overlap = system & gold\n",
    "    score['overlap'] += len(overlap)\n",
    "    score['system'] += len(system)\n",
    "    score['gold'] += len(gold)\n",
    "\n",
    "def print_score(score) :\n",
    "    try :\n",
    "        precision = score['overlap']/score['system']\n",
    "    except :\n",
    "        precision = 0\n",
    "    try :\n",
    "        recall = score['overlap']/score['gold']\n",
    "    except :\n",
    "        recall = 0\n",
    "    try :\n",
    "        fscore = 2 * ((precision * recall) / (precision + recall))\n",
    "    except :\n",
    "        fscore = 0\n",
    "    print('precision:{:.3f}, recall:{:.3f}, fscore:{:.3f}'.format(precision,recall,fscore))\n",
    "\n",
    "def print_spacy_qids(qids,ents) :\n",
    "    for ent in ents :\n",
    "        if  ent[2] in qids :\n",
    "            print(ent)\n",
    "            qids.discard(ent[2])\n",
    "    print(\"\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3717b-2748-41d8-a8db-53e5f0a7e50d",
   "metadata": {},
   "source": [
    "### Evaluation on multiNERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24698fe1-a470-493c-9ce7-2047921723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def filterNERD(tag) :\n",
    "    try :\n",
    "        neclass = tag.split('-')[1]\n",
    "    except :\n",
    "        neclass = 'MISSING'\n",
    "    if neclass not in ['ANIM', 'FOOD', 'DIS', 'PLANT' ] :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def filterSpacy(tags) :\n",
    "    filtered = set()\n",
    "    for tag in tags :\n",
    "        neclass = tag.split('/')[0]\n",
    "        if neclass in ['EVENT','GPE','LOC','ORG','PERSON','DATE','TIME', 'WORK_OF_ART', 'PRODUCT' ] :\n",
    "            filtered.add(tag)\n",
    "    return filtered \n",
    "    # return tags  # no filtering \n",
    "    \n",
    "with open(\"multiNERD/dev2000.tsv\") as nd:\n",
    "    nerd = csv.reader(nd, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    sentence = []\n",
    "    annotation = set()\n",
    "    score = {'overlap':0, 'system' : 0, 'gold' : 0}\n",
    "    for row in nerd:\n",
    "        if len(row) >= 3 :\n",
    "            sentence.append(row[1])\n",
    "            if len(row) > 4 :\n",
    "                if row[4] and filterNERD(row[2]):\n",
    "                    annotation.add(row[2]+'/'+row[4])\n",
    "        else :\n",
    "            string = ' '.join(sentence)\n",
    "            string = string.replace(' - ','-') # Engels - Nederlandse \n",
    "            string = string.replace(' e ','e ') # 19 e eeuw\n",
    "            print(string)\n",
    "            doc = nlp(string)\n",
    "            systemNE = {ent.label_ + '/' + ent._.wikidata_id for ent in doc.ents}\n",
    "            system = filterSpacy(systemNE)\n",
    "            print(systemNE,system,annotation)\n",
    "            update_score( {result.split('/')[1] for result in system}, {result.split('/')[1] for result in annotation}, score)\n",
    "            sentence=[]\n",
    "            annotation = set()\n",
    "    print_score(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84e766-06d8-4970-bd01-61f9a8c27444",
   "metadata": {},
   "source": [
    "over first 1000 lines prec and recall is 0.59 if we apply filtering.\n",
    "including date/time R 0.61 P 0.52 \n",
    "\n",
    "latest version (1000 lines): precision:0.514, recall:0.632\n",
    "over 2000 lines:             precision:0.517, recall:0.602"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73db77-1897-477c-949e-b0d377d5fa22",
   "metadata": {},
   "source": [
    "### Evaluate on WiNNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa8220-8f35-4412-9ba0-b16fe72bd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "winnl = pandas.read_json('WiNNL/dutch_winnl_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a859c4-e3a3-4ad6-aec2-5826719d91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winnl_annotations(row) :\n",
    "    gold = set()\n",
    "    for token in row['qid'] :\n",
    "        if token.startswith('B-') :\n",
    "            qid = token.replace('B-','')\n",
    "            gold.add(qid)\n",
    "    return gold \n",
    "\n",
    "def evaluate_winnl_items(First,Last) :\n",
    "    score = {'overlap':0, 'system' : 0, 'gold' : 0}\n",
    "    for Id in list(range(First,Last)) :\n",
    "        row = winnl.loc[Id]\n",
    "        gold = winnl_annotations(row)\n",
    "        if gold and 'Q404' not in gold : \n",
    "            ## old: ['EVENT','GPE','LOC','ORG','PERSON','DATE','TIME']\n",
    "            system = annotate_text(row['original'], ['LOC','GPE','ORG','PERSON','EVENT'] )\n",
    "            print(system,gold)\n",
    "            update_score(system,gold,score)\n",
    "    print(system & gold)\n",
    "    print(system.difference(system & gold))\n",
    "    print(gold.difference(system & gold))\n",
    "    print_score(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686eae9-2714-4220-ad75-e0cf087d9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_winnl_items(4225,4235)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9da8b2-3dd7-41fa-acdc-d67abf8a42e8",
   "metadata": {},
   "source": [
    "recall:0.445, precision:0.430\n",
    "\n",
    "after strict filtering (loc/gpe/per/org only, and ignore data with 404 or no NE): recall:0.495, precision:0.589\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfb3ea-5827-48b2-9ade-f89e3f948336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_winnl_article(url) : # solve the Obama/Barack Obama issue, ie ensure they point to same QID\n",
    "    entities = {}\n",
    "    score = {'overlap':0, 'system' : 0, 'gold' : 0}\n",
    "    for index, row in winnl.loc[winnl['url'] == url].iterrows() :\n",
    "        entities[index] = find_entities(row['original'])\n",
    "        gold[index] = winnl_annotations(row)\n",
    "    for index in entities :\n",
    "        if gold[index] and 'Q404' not in gold[index] :\n",
    "            system = resolve_entities(index,entities)\n",
    "            update_score(system,gold,score)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6ad6d-d4c2-4d42-b830-e111ddeebfd8",
   "metadata": {},
   "source": [
    "### Evaluate on damuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e97de-a570-4add-bc4a-cbb0abd6c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "damuel = pandas.read_json('damuel_1.0_nl/part-00000', lines=True)\n",
    "\n",
    "import pickle\n",
    "\n",
    "nec_types = pickle.load(open('damuel_1.0_nl/damuel_1.0_wikidata/all_nec_dict.p', \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9de33-78b0-43a9-87b0-878e3508f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def damuel_annotation(wiki) :\n",
    "    links = set()\n",
    "    for link in wiki['links'] :\n",
    "        try :\n",
    "            links.add(link['qid'])\n",
    "        except :\n",
    "            True\n",
    "    annotation = set()\n",
    "    for qid in sorted(links) :\n",
    "        try :\n",
    "            NEC = nec_types[qid]\n",
    "            print(qid,NEC)\n",
    "            annotation.add(qid)\n",
    "        except :\n",
    "            True\n",
    "    return annotation\n",
    "\n",
    "def damuel_annotation_old(wiki) :\n",
    "    annotation = set()\n",
    "    for link in wiki['links'] :\n",
    "        start = link['start']\n",
    "        end  = link['end']\n",
    "        upostags = []\n",
    "        string = []\n",
    "        propn = 0\n",
    "        for token in wiki['tokens'][start:end] : \n",
    "            upostags.append(token['upostag'])\n",
    "            string.append(token['lemma'])\n",
    "            if token['upostag'] == 'PROPN' :\n",
    "                propn = 1\n",
    "        try :\n",
    "            qid = link['qid']\n",
    "        except :\n",
    "            qid = 'missing'\n",
    "        if propn :\n",
    "            annotation.add(qid)\n",
    "    return annotation\n",
    "\n",
    "def print_damuel_annotation(qids,wiki) :\n",
    "    for link in wiki['links'] :\n",
    "        try :\n",
    "            link_id = link['qid']\n",
    "        except :\n",
    "            link_id = 'missing'\n",
    "        if link_id in qids:\n",
    "            start = link['start']\n",
    "            end  = link['end']\n",
    "            upostags = []\n",
    "            string = []\n",
    "            for token in wiki['tokens'][start:end] : \n",
    "                upostags.append(token['upostag'])\n",
    "                string.append(token['lemma'])\n",
    "            print(link_id, string, upostags, link['title'])\n",
    "            qids.discard(link_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c01d9-5915-40e3-9cef-ec39ae6f5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = damuel.loc[95]['wiki']\n",
    "\n",
    "#text = nlp(wiki['text']) \n",
    "#for ent in text.ents :\n",
    "#    print(ent.label_) \n",
    "\n",
    "parse = nlp(wiki['text'])\n",
    "entities = extract_entities(parse)\n",
    "system = system_annotation(entities,['LOC','GPE','ORG','PERSON','EVENT','WORK_OF_ART']) \n",
    "\n",
    "gold = damuel_annotation(wiki)\n",
    "\n",
    "score = {'overlap':0, 'system' : 0, 'gold' : 0}\n",
    "update_score(system,gold,score)\n",
    "\n",
    "#print(system,damuel_annotation(wiki)) \n",
    "print_score(score)\n",
    "print(\"overlap:\")\n",
    "print_spacy_qids(system & gold,entities)\n",
    "print(\"system only:\")\n",
    "print_spacy_qids(system.difference(system & gold),entities)\n",
    "print(\"annotation only:\")\n",
    "print_damuel_annotation(gold.difference(system & gold),wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcc3fa-482e-4a05-b6d5-02aef9573961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_damuel(First,Last) :\n",
    "    score = {'overlap':0, 'system' : 0, 'gold' : 0}\n",
    "    for Id in list(range(First,Last)) :\n",
    "        wiki = damuel.loc[Id]['wiki']\n",
    "        try : \n",
    "            parse = nlp(wiki['text'])\n",
    "            entities = extract_entities(parse)\n",
    "            system = system_annotation(entities,['LOC','GPE','ORG','PERSON','EVENT','WORK_OF_ART','PRODUCT']) \n",
    "            gold = damuel_annotation(wiki)\n",
    "            update_score(system,gold,score)\n",
    "        except :\n",
    "            True\n",
    "    print_score(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad957a1-eca2-4349-b89c-fc61d0996629",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_damuel(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b21ae-ba32-419e-9013-3cf7a637cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
